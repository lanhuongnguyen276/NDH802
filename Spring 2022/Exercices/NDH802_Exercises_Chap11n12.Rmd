---
title: "NDH802 - Regression"
author: "Chapter 11 and 12"
output: 
 prettydoc::html_pretty:
   theme: hpstr
   highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

> *We assign the computation to computers---our tasks are to think, analyze, and make recommendations*" (Newbold, 2016, p. 429).

## 11.5

We fit the linear regression

$y = b_0 + b_1x$

```{r}
#data input
x = c(2,4,3,6,3,5,6,2)
y = c(5,10,8,18,6,15,20,4)

#a. scatter plot
plot(x, y)

#b. calculating the regression coefficients by the book.
b1 = cov(x, y)/(sd(x)^2)
b0 = mean(y) - b1 * mean(x)

#b. calculating the regression coefficients with R shortcut
mod = lm(y ~ x)
summary(mod)

#for fun, we plot the line
plot(x, y, col = "blue"); abline(mod)
```

For learning purpose, we can try calculating some of the components by hand using the formula provided in the lecture slides. `lm()` automatically calculates and report them in the model summary.

```{r}
SSE = sum(mod$residuals^2)
SST=sum((y-mean(y))^2)
n = length(y)
df = mod$df.residual

Residuals = summary(mod$residuals)
Residual_SE = sqrt(SSE/mod$df.residual)
R_squared = 1-SSE/SST
Adjusted_R_squared = 1-(SSE/SST)*(n-1)/df
```

For interpretation of b0 and b1, please refer to the last video on this [page.](https://sse.instructure.com/courses/425/pages/may-17-regression?module_item_id=6183)

#### Prediction

After fitting the model, we can write it as:

$y = -3.5695 + 3.6954*x$

Prediction (in this course) is simply plugging in the new x to the model to find the new y.

```{r}
#the more systematic way. normally, we fit the model many times, and we may want to automatize the process like this
b0 = as.numeric(mod$coefficients[1])
b1 = as.numeric(mod$coefficients[2])

#the simplier way, you copy an paste it from the summary(mod)
b0 = -3.5695
b1 = 3.6954

#For example, we have a new x = 5
newx = 5
#We plug the new x in the model
yPred = b0 + b1 * newx
```

## 11.97

Data description:

![](https://github.com/lanhuongnguyen276/NDH802/blob/master/Exercises/hei1.png?raw=true)

![](https://github.com/lanhuongnguyen276/NDH802/blob/master/Exercises/hei2.png?raw=true)

To investigate the relationship between the HEI score and the daily cost, we estimate this linear model:

$HEI = b_0 + b_1*DailyCost$

```{r}
#data prep
hei <- read.csv("https://raw.githubusercontent.com/lanhuongnguyen276/NDH802/master/Exercises/HEI.csv")[, -1]

x1 = hei[hei$daycode == 1, "daily_cost"] #daily cost, first interview
y1 = hei[hei$daycode == 1, "HEI2005"] #HEI score, first interview
x2 = hei[hei$daycode == 2, "daily_cost"] #daily cost, second interview
y2 = hei[hei$daycode == 2, "HEI2005"] #HEI score, second interview

#Build the regression based on the first interview data
mod1 = lm(y1 ~ x1)
summary(mod1)
#plot the data with the fitted line
plot(x1, y1, xlab = "daily cost", ylab = "HEI score", main = "First interview"); abline(mod1, col = "red", lwd = 2)

#Build the regression based on the second interview data
mod2 = lm(y2 ~ x2)
summary(mod2)
#plot the data with the fitted line
plot(x2, y2, xlab = "daily cost", ylab = "HEI score", main = "Second interview"); abline(mod2, col = "blue", lwd = 2)
```

Alternatively, we can do as follows. Please understand that it is just another way of coding, not thinking. The best way is the way you feel most comfortable with.

```{r}
#afterloading the data, we go straight into fitting the model.
mod1.1 = lm(hei[hei$daycode == 1, "HEI2005"] ~ hei[hei$daycode == 1, "daily_cost"],
            data = hei)
mod2.1 = lm(hei[hei$daycode == 2, "HEI2005"] ~ hei[hei$daycode == 2, "daily_cost"],
            data = hei)
```

## 12.114

This is the example we did in class (which is not part of the question from the book).

$HEI = b_0 + b_1*female + b_2*age$

```{r}
mod_ex = lm(HEI2005 ~ female + age, data = hei)
summary(mod_ex)

#we extract the coefficients
b0_ex = 43.557988
b1_ex = 2.940651
b2_ex = 0.159183

#In class, we take an example that we want to predict the HEI of a new person,
#who is male (0*b1_ex) and 20 years old (20*b2_ex)
yPred_ex = b0_ex + 0*b1_ex + 20*b2_ex
```

a\. Here we are asked to fit the basic linear model:

$HEI = b_0 + b_1*docbp + b_2*waistper + b_3*BMI+ b_4*sroverweight + b_5*female + b_6*age + b_7*daycode$

```{r}
mod_a = lm(HEI2005 ~ doc_bp + waistper + BMI + sr_overweight +
          female + age + daycode, data = hei)
summary(mod_a)
```

b\. We add one more IV, immigrant:

$HEI = b_0 + b_1*docbp + b_2*waistper + b_3*BMI+ b_4*sroverweight + b_5*female + b_6*age + b_7*daycode + b_8*immigrant$

```{r}
mod_b = lm(HEI2005 ~ doc_bp + waistper + BMI + sr_overweight +
          female + age + daycode + immigrant, data = hei)
summary(mod_b)
```

c\. We add one more IV, single:

$HEI = b_0 + b_1*docbp + b_2*waistper + b_3*BMI+ b_4*sroverweight + b_5*female + b_6*age + b_7*daycode + b_8*single$

```{r}
mod_c = lm(HEI2005 ~ doc_bp + waistper + BMI + sr_overweight +
          female + age + daycode + single, data = hei)
summary(mod_c)
```

c\. We add one more IV, fsp:

$HEI = b_0 + b_1*docbp + b_2*waistper + b_3*BMI+ b_4*sroverweight + b_5*female + b_6*age + b_7*daycode + b_8*fsp$

```{r}
mod_d = lm(HEI2005 ~ doc_bp + waistper + BMI + sr_overweight +
          female + age + daycode + fsp, data = hei)
summary(mod_d)
```

## 12.107

Data description:

![](https://github.com/lanhuongnguyen276/NDH802/blob/master/Exercises/salary.png?raw=true){width="634"}

```{r}
#data prep
salary <- read.csv("https://raw.githubusercontent.com/lanhuongnguyen276/NDH802/master/Exercises/Salary_Study.csv")[, -1]

#It is important to learn what we should include in the model
#It is equally important to learn what we should NOT include in the model

#we may not want to include the IVs that are highly correlated (multicollinearity)
#for example, age and Experience
cor(salary)
#If you want to learn more about multicollinearity -> Newbold chapter 13.

#This is an example. Adjust it as you reason
mod.12107 = lm(Salary ~ Gender + Experience + Market, data = salary)
summary(mod.12107)
```
