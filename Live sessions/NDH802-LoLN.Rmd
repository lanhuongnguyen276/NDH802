---
title: "Law of large number at a glance"
author: "NDH802 R Application"
output: html_document
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#setwd("~/Documents/GitHub/NDH802")
if(!require(shiny)){
    install.packages("shiny")
    library(shiny)
}
```

<br>

## Visual Illustrations

Below are some demos of some of the distributions we have learned in this course. You can adjust the parameters and sample size and observe the changes in the sample mean and the probability distribution.

*Note*: Don't freak out, the code is not part of this course. Just hit Run Document (where the Knit button usually is) and have fun with the interactive html doc!

### Bernoulli distribution

```{r bern, echo=FALSE, out.width=0.1}
sliderInput(
  "p_bern",
  label = "Parameter p:",
  min = 0,
  max = 1,
  value = 0.5,
  step = 0.01
)
sliderInput(
  "n_bern",
  label = "Number of observations:",
  min = 1,
  max = 10000,
  value = 100,
  step = 1
)

#set.seed(2706)
renderPlot(barplot(
  table(rbinom(input$n_bern, 1, input$p_bern)),
  main = paste(
    "Bernoulli distribution with",
    input$n_bern,
    "observation(s).",
    "Sample mean =",
    round(mean(rbinom(input$n_bern, 1, input$p_bern)), digits = 5)
  ),
  xlab = "", col = "lightblue3", border = F,
))
```

### Binomial distribution

Note: For illustration purpose, number of trials is fixed at 100.

```{r bin, echo=FALSE, out.width=0.1}
sliderInput(
  "p_bin",
  label = "Parameter p:",
  min = 0,
  max = 1,
  value = 0.5,
  step = 0.01
)
sliderInput(
  "n_bin",
  label = "Number of observations:",
  min = 1,
  max = 10000,
  value = 100,
  step = 1
)

#set.seed(2706)
renderPlot(barplot(
  table(rbinom(input$n_bin, 100, input$p_bin)),
  main = paste(
    "Binomial distribution with 100 trials and",
    input$n_bin,
    "observation(s).",
    "Sample mean =",
    round(mean(rbinom(input$n_bin, 100, input$p_bin)), digits = 5)
  ),
  xlab = "", col = "darkseagreen", border = F,
))
```

### Standard normal distribution

```{r norm, echo=FALSE}
sliderInput(
  "n_norm",
  label = "Number of observations:",
  min = 1,
  max = 10000,
  value = 1000,
  step = 1
)

renderPlot(hist(
  rnorm(input$n_norm), breaks = 50,
  main = paste(
    "Standard normal distribution with",
    input$n_norm,
    "observation(s).",
    "Sample mean =",
    round(mean(rnorm(input$n_norm)), digits = 5)
  ), xlab = "", col = "pink3", border = F,
))
```

## Key takeaways

**Notice that sample mean** $\bar{X}$ **gets closer and closer to the population mean when the number of observation** $n$ **increases.** **However,** $\bar{X}$ **will *never* be equal to** $\mu$**.** If you're wondering how large is *large* and how close is *close*, check out the Central Limit Theorem.

## Why do we care?

The Law of Large numbers and the Central Limit Theorem are the foundations of statistics. One application is that they help us to say something about the population (which is normally unknown) through the sample data.

If you're not a fan of technical terms, let us consider a simple real life example. Imagine you go to a casino and place bet. The bet is 50 kr. The dealer then flips a coin. If it is head, you win (additionally) 25kr, if it is tail, you lose the bet. The probability of getting a head is 60%. The probability of winning is higher than losing, then it seems like the game is in our favor, right? Let us give it a second thought and calculate the expected value:

$$
E(X) = \text{wining value}*P(\text{you win}) + \text{losing value}*P(\text{you lose})
 = 25*0.6+(-50)*0.4 = -5 
$$

**What does this negative value mean?** It means if the player places the bet many many times, (s)he is going to lose. Luckily for the player, unless (s)he plays many many times (which is highly unlikely), the LLN won't apply to the single player.

**Then?** The LLN, however, applies to the casino, as there are a lot of players (i.e., the big sample size $n$ we discussed). If a player loses 5kr, it means the casino makes 5kr. If there are, say 1,000,000 players, the casino makes 5,000,000 kr, easy peasy.

**Will knowing the LLN certainly prevent you, as a player, from losing money?** Nope, gambling is probabilistic, and the best you can do is to increase/decrease the chance that you win/lose. But at least now you know you can't outsmart the casinos, in the long run ðŸ˜ª.

*Huong*

```{r}
dbinom(10,10,0.5)
(2^30)
```
